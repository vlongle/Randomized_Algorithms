{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* https://en.wikipedia.org/wiki/Randomized_algorithm\n",
    "* https://people.cs.umass.edu/~mcgregor/CS690RA20/index.html\n",
    "* https://polylogblog.wordpress.com/2012/03/29/a-booles-errand/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "In deterministic algorithms, for a fixed input $x$, the output is $f(x)$, only depending on the inputs. In randomized algorithms, the output is $f(x, c)$, depending on both the input and the random bit $c$. Thus, we can get two different answers on the same input for randomized algorithm while we always get the same answer in deterministic. \n",
    "\n",
    "### Randomized algs vs probabilistic analysis\n",
    "In randomized alg, the expectations and probabilities and taken over the random bit $c$. So when we say the probability of success is $90\\%$, this means it works for 90% of the random bits. This probability works for _every_ inputs, that is it works _uniformly_ over all $x$.\n",
    "\n",
    "In probabilistic analysis, we explicitly assume a distribution over the incoming data $x$ and analyze alg's performance based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Algorithms\n",
    "Consider the toy problem from Wiki. We have an array $x$ always guaranteed to contain half 0 and half 1. The problem is to find any index $i$ such that $x[i]=0$. A randomized algorithm goes like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def find_zero(x):\n",
    "    # pick a random index uniformly and return that\n",
    "    c = random.randint(0, len(x)-1) \n",
    "    return c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any input $x$, this algorithm will return the correct result (succeeds) half of the time. From a practical perspective, if we say the algorithm succeeds with prob $p$ for any given input, then we expect $np$ successes with $n$ problems to solve. $np$ is mean of binomial distribution. And the fraction of successes converges to the mean under the Law of Large Numbers for $n$ large enough.\n",
    "\n",
    "Now, we have an intuitive interpretation. Say $p=0.95$. It means that out of billion problems, we can solve 950 millions of them correctly. Does that sound good? Does the algorithm also use less space and time? If yes, then it's a good algorithm for your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo vs Las Vegas\n",
    "Formally, we distinguish between two types of random algo. \n",
    "* Monte Carlo (MC)\n",
    "* Las Vegas\n",
    "based on their randomness. In MC, the running time is bounded and finite. But the answer might be wrong (with small probability).\n",
    "\n",
    "In Las Vegas, the algorithm always returns the correct answer. But the running time might be infinite. The running time is a random variable whose expectation is usually constrained to be bounded. Again, an example from Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of Las Vegas\n",
    "def find_zero_LV(x):\n",
    "    while True: # might run forever\n",
    "        c = random.randint(0, len(x)-1)\n",
    "        if x[c] == 0:\n",
    "            return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected running time is the mean of geometric random variable. We expect to run 2 trials but we might run forever if we're unlucky. Of course, in practice, we should always have a time-out to shut down the unlucky cases and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip Lists\n",
    "* https://eugene-eeo.github.io/blog/skip-lists.html\n",
    "* https://15721.courses.cs.cmu.edu/spring2018/papers/08-oltpindexes1/pugh-skiplists-cacm1990.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1: Intro\n",
    "https://people.cs.umass.edu/~mcgregor/690RAS20/lec-intro.pdf\n",
    "\n",
    "Prof. McGregor\n",
    "\n",
    "### Randomized algorithms: the good and the bad\n",
    "The good\n",
    "* __Simple__: some random algs are extremely simple and we can boost success prob very high\n",
    "* __Speed__: sublinear time alg\n",
    "* __Against adversary__: apparently good against opponents (not sure about this though ...)\n",
    "* We gotta do something. Some problems are __NP__ (solving exactly and optimally requires brute-force search) so randomized/approximation algs are the best things out there. \n",
    "\n",
    "The bad: randomized algs are, well, random so either running time or correctness are not guaranteed 100%. Also, it's hard to debug.\n",
    "\n",
    "## Topics\n",
    "* __Classic__: solving NP problems like rounding of linear programs. MC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
